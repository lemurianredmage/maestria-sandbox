{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network\n",
    "\n",
    "Start-of-the-art tattoo verification using Siamese Network\n",
    "\n",
    "#### References\n",
    "- [Siamese Network](https://builtin.com/machine-learning/siamese-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will help us to measure the time it took for the whole\n",
    "# notebook to execute\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../utils')\n",
    "import datasets\n",
    "importlib.reload(datasets)\n",
    "import helpers\n",
    "importlib.reload(helpers)\n",
    "import annotations\n",
    "importlib.reload(annotations)\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base images in data folder: \n",
      "     Total of bounding boxes:  4410\n",
      "     Total of images:  4410\n",
      "     Total of base images:  161\n",
      "\n",
      "Base images and their variants\n",
      "    Base image '118_1.JPG' has 21 variants.\n",
      "    Base image '103_2.JPG' has 21 variants.\n",
      "    Base image '77_1.JPG' has 21 variants.\n",
      "    Base image '32_1.JPG' has 21 variants.\n",
      "    Base image '14_2.JPG' has 21 variants.\n",
      "    Base image '53_1.JPG' has 21 variants.\n",
      "    Base image '16_1.JPG' has 21 variants.\n",
      "    Base image '144_1.JPG' has 21 variants.\n",
      "    Base image '93_1.JPG' has 21 variants.\n",
      "    Base image '91_1.JPG' has 21 variants.\n",
      "    Base image '146_1.JPG' has 21 variants.\n",
      "    Base image '103_1.JPG' has 21 variants.\n",
      "    Base image '51_1.JPG' has 21 variants.\n",
      "    Base image '14_1.JPG' has 21 variants.\n",
      "    Base image '88_1.JPG' has 21 variants.\n",
      "    Base image '75_1.JPG' has 21 variants.\n",
      "    Base image '16_2.JPG' has 21 variants.\n",
      "    Base image '127_1.JPG' has 21 variants.\n",
      "    Base image '48_1.JPG' has 21 variants.\n",
      "    Base image '10_4.JPG' has 21 variants.\n",
      "    Base image '71_1.JPG' has 21 variants.\n",
      "    Base image '34_1.JPG' has 21 variants.\n",
      "    Base image '12_2.JPG' has 21 variants.\n",
      "    Base image '123_1.JPG' has 21 variants.\n",
      "    Base image '68_1.JPG' has 21 variants.\n",
      "    Base image '95_1.JPG' has 21 variants.\n",
      "    Base image '12_3.JPG' has 21 variants.\n",
      "    Base image '10_1.JPG' has 21 variants.\n",
      "    Base image '10_3.JPG' has 21 variants.\n",
      "    Base image '12_1.JPG' has 21 variants.\n",
      "    Base image '34_2.JPG' has 21 variants.\n",
      "    Base image '105_1.JPG' has 21 variants.\n",
      "    Base image '97_1.JPG' has 21 variants.\n",
      "    Base image '138_1.JPG' has 21 variants.\n",
      "    Base image '159_1.JPG' has 21 variants.\n",
      "    Base image '121_1.JPG' has 21 variants.\n",
      "    Base image '73_1.JPG' has 21 variants.\n",
      "    Base image '36_1.JPG' has 21 variants.\n",
      "    Base image '10_2.JPG' has 21 variants.\n",
      "    Base image '17_3.JPG' has 21 variants.\n",
      "    Base image '102_1.JPG' has 21 variants.\n",
      "    Base image '9_3.JPG' has 21 variants.\n",
      "    Base image '147_1.JPG' has 21 variants.\n",
      "    Base image '15_1.JPG' has 21 variants.\n",
      "    Base image '50_1.JPG' has 21 variants.\n",
      "    Base image '90_1.JPG' has 21 variants.\n",
      "    Base image '11_5.JPG' has 21 variants.\n",
      "    Base image '49_1.JPG' has 21 variants.\n",
      "    Base image '11_4.JPG' has 21 variants.\n",
      "    Base image '89_1.JPG' has 21 variants.\n",
      "    Base image '31_1.JPG' has 21 variants.\n",
      "    Base image '74_1.JPG' has 21 variants.\n",
      "    Base image '17_2.JPG' has 21 variants.\n",
      "    Base image '126_1.JPG' has 21 variants.\n",
      "    Base image '124_1.JPG' has 21 variants.\n",
      "    Base image '33_1.JPG' has 21 variants.\n",
      "    Base image '76_1.JPG' has 21 variants.\n",
      "    Base image '15_2.JPG' has 21 variants.\n",
      "    Base image '119_1.JPG' has 21 variants.\n",
      "    Base image '11_6.JPG' has 21 variants.\n",
      "    Base image '92_1.JPG' has 21 variants.\n",
      "    Base image '15_3.JPG' has 21 variants.\n",
      "    Base image '9_1.JPG' has 21 variants.\n",
      "    Base image '17_1.JPG' has 42 variants.\n",
      "    Base image '52_1.JPG' has 21 variants.\n",
      "    Base image '145_1.JPG' has 21 variants.\n",
      "    Base image '17_5.JPG' has 21 variants.\n",
      "    Base image '9_5.JPG' has 21 variants.\n",
      "    Base image '96_1.JPG' has 21 variants.\n",
      "    Base image '13_1.JPG' has 21 variants.\n",
      "    Base image '56_1.JPG' has 21 variants.\n",
      "    Base image '104_1.JPG' has 21 variants.\n",
      "    Base image '11_3.JPG' has 21 variants.\n",
      "    Base image '120_1.JPG' has 21 variants.\n",
      "    Base image '37_1.JPG' has 21 variants.\n",
      "    Base image '11_2.JPG' has 21 variants.\n",
      "    Base image '9_4.JPG' has 21 variants.\n",
      "    Base image '158_1.JPG' has 21 variants.\n",
      "    Base image '17_4.JPG' has 21 variants.\n",
      "    Base image '17_6.JPG' has 21 variants.\n",
      "    Base image '15_4.JPG' has 21 variants.\n",
      "    Base image '35_1.JPG' has 21 variants.\n",
      "    Base image '70_1.JPG' has 21 variants.\n",
      "    Base image '13_2.JPG' has 21 variants.\n",
      "    Base image '122_1.JPG' has 21 variants.\n",
      "    Base image '11_1.JPG' has 21 variants.\n",
      "    Base image '54_1.JPG' has 21 variants.\n",
      "    Base image '37_2.JPG' has 21 variants.\n",
      "    Base image '15_5.JPG' has 21 variants.\n",
      "    Base image '69_1.JPG' has 21 variants.\n",
      "    Base image '17_7.JPG' has 21 variants.\n",
      "    Base image '65_1.JPG' has 21 variants.\n",
      "    Base image '20_1.JPG' has 21 variants.\n",
      "    Base image '137_1.JPG' has 21 variants.\n",
      "    Base image '98_1.JPG' has 21 variants.\n",
      "    Base image '58_1.JPG' has 21 variants.\n",
      "    Base image '39_1.JPG' has 21 variants.\n",
      "    Base image '81_1.JPG' has 21 variants.\n",
      "    Base image '1_2.JPG' has 21 variants.\n",
      "    Base image '156_1.JPG' has 21 variants.\n",
      "    Base image '113_1.JPG' has 21 variants.\n",
      "    Base image '22_2.JPG' has 21 variants.\n",
      "    Base image '41_1.JPG' has 21 variants.\n",
      "    Base image '17_9.JPG' has 21 variants.\n",
      "    Base image '20_2.JPG' has 21 variants.\n",
      "    Base image '43_1.JPG' has 21 variants.\n",
      "    Base image '154_1.JPG' has 21 variants.\n",
      "    Base image '111_1.JPG' has 21 variants.\n",
      "    Base image '83_1.JPG' has 21 variants.\n",
      "    Base image '1_1.JPG' has 21 variants.\n",
      "    Base image '135_1.JPG' has 21 variants.\n",
      "    Base image '22_1.JPG' has 21 variants.\n",
      "    Base image '17_8.JPG' has 21 variants.\n",
      "    Base image '24_2.JPG' has 21 variants.\n",
      "    Base image '47_1.JPG' has 21 variants.\n",
      "    Base image '115_1.JPG' has 21 variants.\n",
      "    Base image '19_2.JPG' has 21 variants.\n",
      "    Base image '128_1.JPG' has 21 variants.\n",
      "    Base image '87_1.JPG' has 21 variants.\n",
      "    Base image '85_1.JPG' has 21 variants.\n",
      "    Base image '78_1.JPG' has 21 variants.\n",
      "    Base image '152_1.JPG' has 21 variants.\n",
      "    Base image '117_1.JPG' has 21 variants.\n",
      "    Base image '45_1.JPG' has 21 variants.\n",
      "    Base image '61_1.JPG' has 21 variants.\n",
      "    Base image '24_1.JPG' has 21 variants.\n",
      "    Base image '19_1.JPG' has 21 variants.\n",
      "    Base image '82_1.JPG' has 21 variants.\n",
      "    Base image '21_2.JPG' has 21 variants.\n",
      "    Base image '110_1.JPG' has 21 variants.\n",
      "    Base image '155_1.JPG' has 21 variants.\n",
      "    Base image '134_1.JPG' has 21 variants.\n",
      "    Base image '66_1.JPG' has 21 variants.\n",
      "    Base image '21_3.JPG' has 21 variants.\n",
      "    Base image '80_2.JPG' has 21 variants.\n",
      "    Base image '2_1.JPG' has 21 variants.\n",
      "    Base image '21_1.JPG' has 21 variants.\n",
      "    Base image '64_1.JPG' has 21 variants.\n",
      "    Base image '136_1.JPG' has 21 variants.\n",
      "    Base image '99_1.JPG' has 21 variants.\n",
      "    Base image '112_1.JPG' has 21 variants.\n",
      "    Base image '157_1.JPG' has 21 variants.\n",
      "    Base image '40_1.JPG' has 21 variants.\n",
      "    Base image '38_1.JPG' has 21 variants.\n",
      "    Base image '80_1.JPG' has 21 variants.\n",
      "    Base image '116_1.JPG' has 21 variants.\n",
      "    Base image '153_1.JPG' has 21 variants.\n",
      "    Base image '17_10.JPG' has 21 variants.\n",
      "    Base image '44_1.JPG' has 21 variants.\n",
      "    Base image '84_1.JPG' has 21 variants.\n",
      "    Base image '21_4.JPG' has 21 variants.\n",
      "    Base image '79_1.JPG' has 21 variants.\n",
      "    Base image '6_1.JPG' has 21 variants.\n",
      "    Base image '132_1.JPG' has 21 variants.\n",
      "    Base image '130_1.JPG' has 21 variants.\n",
      "    Base image '148_1.JPG' has 21 variants.\n",
      "    Base image '4_1.JPG' has 21 variants.\n",
      "    Base image '129_1.JPG' has 21 variants.\n",
      "    Base image '86_1.JPG' has 21 variants.\n",
      "    Base image '114_1.JPG' has 21 variants.\n",
      "    Base image '151_1.JPG' has 21 variants.\n",
      "\n",
      "Annotations file saved as 'annotations.csv' with 127890 pairs.\n"
     ]
    }
   ],
   "source": [
    "bound_box_path = Path(\"../../datasets/BIVTatt-Dataset/bounding_boxes\")\n",
    "data_path = Path(\"../../datasets/BIVTatt-Dataset/images\")\n",
    "pattern = r'^\\d+_\\d+\\.JPG'\n",
    "total_bound_boxes = [file.name for file in bound_box_path.iterdir() if file.is_file()]\n",
    "all_images = [file.name for file in data_path.iterdir() if file.is_file()]\n",
    "base_images = [file.name for file in data_path.iterdir() if file.is_file() and re.match(pattern, file.name)]\n",
    "\n",
    "print (\"Base images in data folder: \")\n",
    "print(\"     Total of bounding boxes: \", len(total_bound_boxes))\n",
    "print(\"     Total of images: \", len(all_images))\n",
    "print(\"     Total of base images: \", len(base_images))\n",
    "print('')\n",
    "print(\"Base images and their variants\")\n",
    "\n",
    "base_image_variant_counts = {base_image: 0 for base_image in base_images}\n",
    "\n",
    "for image in all_images:\n",
    "    for base_image in base_images:\n",
    "        if image.startswith(base_image[:-4]):\n",
    "            base_image_variant_counts[base_image] += 1\n",
    "\n",
    "for base_image, count in base_image_variant_counts.items():\n",
    "    print(f\"    Base image '{base_image}' has {count} variants.\")\n",
    "\n",
    "# Define dataset base path\n",
    "dataset_path = annotations.bivtatt_dataset_path\n",
    "\n",
    "print(\"\")\n",
    "annotations.bivtatt_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Needed classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class BIVTattDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations_file (str): Path to the annotations CSV file.\n",
    "            img_dir (str): Directory containing tattoo images.\n",
    "            transform (callable, optional): Transformations to be applied on the images.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the pair of images and their label\n",
    "        img1_name = self.annotations.iloc[index]['image1']\n",
    "        img2_name = self.annotations.iloc[index]['image2']\n",
    "        label = self.annotations.iloc[index]['label']\n",
    "\n",
    "        # Load images\n",
    "        img1 = Image.open(os.path.join(self.img_dir, img1_name)).convert(\"RGB\")\n",
    "        img2 = Image.open(os.path.join(self.img_dir, img2_name)).convert(\"RGB\")\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Dataset location\n",
    "annotations_path = f'{dataset_path}annotations.csv'\n",
    "images_path = f'{dataset_path}images'\n",
    "\n",
    "# Split dataset\n",
    "annotations = pd.read_csv(annotations_path)\n",
    "train_annotations, test_annotations = train_test_split(annotations, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the split annotations\n",
    "train_annotations.to_csv(f'{dataset_path}train_annotations.csv', index=False)\n",
    "test_annotations.to_csv(f'{dataset_path}test_annotations.csv', index=False)\n",
    "\n",
    "train_dataset = BIVTattDataset(annotations_file=f'{dataset_path}train_annotations.csv', img_dir=images_path, transform=transform)\n",
    "test_dataset = BIVTattDataset(annotations_file=f'{dataset_path}test_annotations.csv', img_dir=images_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Siamese Network\n",
    "\n",
    "A siamese neural network (SNN) is a class of neural network architectures that contain two or more identical sub-networks.\n",
    "\n",
    "“Identical” here means they have the same configuration with the same parameters and weights.\n",
    "\n",
    "Parameter updating is mirrored across both sub-networks and it’s used to find similarities between inputs by comparing its feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Use a pre-trained ResNet backbone\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        # Replace the final layer with a smaller output for embeddings\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, 128)\n",
    "    \n",
    "    def forward_once(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def forward(self, img1, img2):\n",
    "        # Pass both images through the backbone\n",
    "        feat1 = self.forward_once(img1)\n",
    "        feat2 = self.forward_once(img2)\n",
    "        return feat1, feat2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Contrastive Loss\n",
    "\n",
    "It’s a distance-based loss as opposed to more conventional error-prediction loss.\n",
    "\n",
    "This loss function is used to learn embeddings in which two similar points have a low Euclidean distance and two dissimilar points have a large Euclidean distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, output1, output2, label):\n",
    "        # Compute the Euclidean distance\n",
    "        distance = torch.norm(output1 - output2, p=2, dim=1)\n",
    "        # Contrastive loss formula\n",
    "        loss = torch.mean(\n",
    "            (1 - label) * torch.pow(distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/administrator/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/administrator/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/administrator/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:02<00:00, 19.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Model\n",
    "model = SiameseNetwork()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1635.5135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m output1, output2 \u001b[38;5;241m=\u001b[39m model(img1, img2)\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output1, output2, label)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for img1, img2, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img1, img2)\n",
    "        loss = criterion(output1, output2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "saved_model = f'{dataset_path}siamese_tattoo_model.pth'\n",
    "torch.save(model.state_dict(), saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def verify_tattoo(img1_path, img2_path, model, transform):\n",
    "    img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "    img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "    img1 = transform(img1).unsqueeze(0)  # Add batch dimension\n",
    "    img2 = transform(img2).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        feat1, feat2 = model(img1, img2)\n",
    "        distance = torch.norm(feat1 - feat2, p=2).item()\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/c9fncrrx69v6k8ww3v84msl80000gn/T/ipykernel_9361/2417322084.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(saved_model))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (backbone): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Model for Testing\n",
    "model.load_state_dict(torch.load(saved_model))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between test images: 0.045044515281915665\n"
     ]
    }
   ],
   "source": [
    "# Test with Example Pairs\n",
    "test_img1 = os.path.join(images_path, '1_2.JPG')\n",
    "test_img2 = os.path.join(images_path, '4_2.JPG')\n",
    "distance = verify_tattoo(test_img1, test_img2, model, transform)\n",
    "print(f\"Distance between test images: {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Total Time\n",
    "\n",
    "This show the total time of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time: 124.0 minutes and 3.15 seconds\n"
     ]
    }
   ],
   "source": [
    "# Sets the total time of execution\n",
    "end_time = time.time()\n",
    "helpers.calculate_execution_time(start_time, end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maestria-sandbox-M6nm_6C6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
