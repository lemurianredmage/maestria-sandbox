{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile Net v2\n",
    "\n",
    "Start-of-the-art tattoo verification using Siamese Network\n",
    "\n",
    "#### References\n",
    "- [MobileNet v2](https://keras.io/api/applications/mobilenet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will help us to measure the time it took for the whole\n",
    "# notebook to execute\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../utils')\n",
    "import datasets\n",
    "importlib.reload(datasets)\n",
    "import helpers\n",
    "importlib.reload(helpers)\n",
    "import annotations\n",
    "importlib.reload(annotations)\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base images in data folder: \n",
      "     Total of bounding boxes:  4410\n",
      "     Total of images:  4410\n",
      "     Total of base images:  161\n",
      "\n",
      "Base images and their variants\n",
      "    Base image '118_1.JPG' has 21 variants.\n",
      "    Base image '103_2.JPG' has 21 variants.\n",
      "    Base image '77_1.JPG' has 21 variants.\n",
      "    Base image '32_1.JPG' has 21 variants.\n",
      "    Base image '14_2.JPG' has 21 variants.\n",
      "    Base image '53_1.JPG' has 21 variants.\n",
      "    Base image '16_1.JPG' has 21 variants.\n",
      "    Base image '144_1.JPG' has 21 variants.\n",
      "    Base image '93_1.JPG' has 21 variants.\n",
      "    Base image '91_1.JPG' has 21 variants.\n",
      "    Base image '146_1.JPG' has 21 variants.\n",
      "    Base image '103_1.JPG' has 21 variants.\n",
      "    Base image '51_1.JPG' has 21 variants.\n",
      "    Base image '14_1.JPG' has 21 variants.\n",
      "    Base image '88_1.JPG' has 21 variants.\n",
      "    Base image '75_1.JPG' has 21 variants.\n",
      "    Base image '16_2.JPG' has 21 variants.\n",
      "    Base image '127_1.JPG' has 21 variants.\n",
      "    Base image '48_1.JPG' has 21 variants.\n",
      "    Base image '10_4.JPG' has 21 variants.\n",
      "    Base image '71_1.JPG' has 21 variants.\n",
      "    Base image '34_1.JPG' has 21 variants.\n",
      "    Base image '12_2.JPG' has 21 variants.\n",
      "    Base image '123_1.JPG' has 21 variants.\n",
      "    Base image '68_1.JPG' has 21 variants.\n",
      "    Base image '95_1.JPG' has 21 variants.\n",
      "    Base image '12_3.JPG' has 21 variants.\n",
      "    Base image '10_1.JPG' has 21 variants.\n",
      "    Base image '10_3.JPG' has 21 variants.\n",
      "    Base image '12_1.JPG' has 21 variants.\n",
      "    Base image '34_2.JPG' has 21 variants.\n",
      "    Base image '105_1.JPG' has 21 variants.\n",
      "    Base image '97_1.JPG' has 21 variants.\n",
      "    Base image '138_1.JPG' has 21 variants.\n",
      "    Base image '159_1.JPG' has 21 variants.\n",
      "    Base image '121_1.JPG' has 21 variants.\n",
      "    Base image '73_1.JPG' has 21 variants.\n",
      "    Base image '36_1.JPG' has 21 variants.\n",
      "    Base image '10_2.JPG' has 21 variants.\n",
      "    Base image '17_3.JPG' has 21 variants.\n",
      "    Base image '102_1.JPG' has 21 variants.\n",
      "    Base image '9_3.JPG' has 21 variants.\n",
      "    Base image '147_1.JPG' has 21 variants.\n",
      "    Base image '15_1.JPG' has 21 variants.\n",
      "    Base image '50_1.JPG' has 21 variants.\n",
      "    Base image '90_1.JPG' has 21 variants.\n",
      "    Base image '11_5.JPG' has 21 variants.\n",
      "    Base image '49_1.JPG' has 21 variants.\n",
      "    Base image '11_4.JPG' has 21 variants.\n",
      "    Base image '89_1.JPG' has 21 variants.\n",
      "    Base image '31_1.JPG' has 21 variants.\n",
      "    Base image '74_1.JPG' has 21 variants.\n",
      "    Base image '17_2.JPG' has 21 variants.\n",
      "    Base image '126_1.JPG' has 21 variants.\n",
      "    Base image '124_1.JPG' has 21 variants.\n",
      "    Base image '33_1.JPG' has 21 variants.\n",
      "    Base image '76_1.JPG' has 21 variants.\n",
      "    Base image '15_2.JPG' has 21 variants.\n",
      "    Base image '119_1.JPG' has 21 variants.\n",
      "    Base image '11_6.JPG' has 21 variants.\n",
      "    Base image '92_1.JPG' has 21 variants.\n",
      "    Base image '15_3.JPG' has 21 variants.\n",
      "    Base image '9_1.JPG' has 21 variants.\n",
      "    Base image '17_1.JPG' has 42 variants.\n",
      "    Base image '52_1.JPG' has 21 variants.\n",
      "    Base image '145_1.JPG' has 21 variants.\n",
      "    Base image '17_5.JPG' has 21 variants.\n",
      "    Base image '9_5.JPG' has 21 variants.\n",
      "    Base image '96_1.JPG' has 21 variants.\n",
      "    Base image '13_1.JPG' has 21 variants.\n",
      "    Base image '56_1.JPG' has 21 variants.\n",
      "    Base image '104_1.JPG' has 21 variants.\n",
      "    Base image '11_3.JPG' has 21 variants.\n",
      "    Base image '120_1.JPG' has 21 variants.\n",
      "    Base image '37_1.JPG' has 21 variants.\n",
      "    Base image '11_2.JPG' has 21 variants.\n",
      "    Base image '9_4.JPG' has 21 variants.\n",
      "    Base image '158_1.JPG' has 21 variants.\n",
      "    Base image '17_4.JPG' has 21 variants.\n",
      "    Base image '17_6.JPG' has 21 variants.\n",
      "    Base image '15_4.JPG' has 21 variants.\n",
      "    Base image '35_1.JPG' has 21 variants.\n",
      "    Base image '70_1.JPG' has 21 variants.\n",
      "    Base image '13_2.JPG' has 21 variants.\n",
      "    Base image '122_1.JPG' has 21 variants.\n",
      "    Base image '11_1.JPG' has 21 variants.\n",
      "    Base image '54_1.JPG' has 21 variants.\n",
      "    Base image '37_2.JPG' has 21 variants.\n",
      "    Base image '15_5.JPG' has 21 variants.\n",
      "    Base image '69_1.JPG' has 21 variants.\n",
      "    Base image '17_7.JPG' has 21 variants.\n",
      "    Base image '65_1.JPG' has 21 variants.\n",
      "    Base image '20_1.JPG' has 21 variants.\n",
      "    Base image '137_1.JPG' has 21 variants.\n",
      "    Base image '98_1.JPG' has 21 variants.\n",
      "    Base image '58_1.JPG' has 21 variants.\n",
      "    Base image '39_1.JPG' has 21 variants.\n",
      "    Base image '81_1.JPG' has 21 variants.\n",
      "    Base image '1_2.JPG' has 21 variants.\n",
      "    Base image '156_1.JPG' has 21 variants.\n",
      "    Base image '113_1.JPG' has 21 variants.\n",
      "    Base image '22_2.JPG' has 21 variants.\n",
      "    Base image '41_1.JPG' has 21 variants.\n",
      "    Base image '17_9.JPG' has 21 variants.\n",
      "    Base image '20_2.JPG' has 21 variants.\n",
      "    Base image '43_1.JPG' has 21 variants.\n",
      "    Base image '154_1.JPG' has 21 variants.\n",
      "    Base image '111_1.JPG' has 21 variants.\n",
      "    Base image '83_1.JPG' has 21 variants.\n",
      "    Base image '1_1.JPG' has 21 variants.\n",
      "    Base image '135_1.JPG' has 21 variants.\n",
      "    Base image '22_1.JPG' has 21 variants.\n",
      "    Base image '17_8.JPG' has 21 variants.\n",
      "    Base image '24_2.JPG' has 21 variants.\n",
      "    Base image '47_1.JPG' has 21 variants.\n",
      "    Base image '115_1.JPG' has 21 variants.\n",
      "    Base image '19_2.JPG' has 21 variants.\n",
      "    Base image '128_1.JPG' has 21 variants.\n",
      "    Base image '87_1.JPG' has 21 variants.\n",
      "    Base image '85_1.JPG' has 21 variants.\n",
      "    Base image '78_1.JPG' has 21 variants.\n",
      "    Base image '152_1.JPG' has 21 variants.\n",
      "    Base image '117_1.JPG' has 21 variants.\n",
      "    Base image '45_1.JPG' has 21 variants.\n",
      "    Base image '61_1.JPG' has 21 variants.\n",
      "    Base image '24_1.JPG' has 21 variants.\n",
      "    Base image '19_1.JPG' has 21 variants.\n",
      "    Base image '82_1.JPG' has 21 variants.\n",
      "    Base image '21_2.JPG' has 21 variants.\n",
      "    Base image '110_1.JPG' has 21 variants.\n",
      "    Base image '155_1.JPG' has 21 variants.\n",
      "    Base image '134_1.JPG' has 21 variants.\n",
      "    Base image '66_1.JPG' has 21 variants.\n",
      "    Base image '21_3.JPG' has 21 variants.\n",
      "    Base image '80_2.JPG' has 21 variants.\n",
      "    Base image '2_1.JPG' has 21 variants.\n",
      "    Base image '21_1.JPG' has 21 variants.\n",
      "    Base image '64_1.JPG' has 21 variants.\n",
      "    Base image '136_1.JPG' has 21 variants.\n",
      "    Base image '99_1.JPG' has 21 variants.\n",
      "    Base image '112_1.JPG' has 21 variants.\n",
      "    Base image '157_1.JPG' has 21 variants.\n",
      "    Base image '40_1.JPG' has 21 variants.\n",
      "    Base image '38_1.JPG' has 21 variants.\n",
      "    Base image '80_1.JPG' has 21 variants.\n",
      "    Base image '116_1.JPG' has 21 variants.\n",
      "    Base image '153_1.JPG' has 21 variants.\n",
      "    Base image '17_10.JPG' has 21 variants.\n",
      "    Base image '44_1.JPG' has 21 variants.\n",
      "    Base image '84_1.JPG' has 21 variants.\n",
      "    Base image '21_4.JPG' has 21 variants.\n",
      "    Base image '79_1.JPG' has 21 variants.\n",
      "    Base image '6_1.JPG' has 21 variants.\n",
      "    Base image '132_1.JPG' has 21 variants.\n",
      "    Base image '130_1.JPG' has 21 variants.\n",
      "    Base image '148_1.JPG' has 21 variants.\n",
      "    Base image '4_1.JPG' has 21 variants.\n",
      "    Base image '129_1.JPG' has 21 variants.\n",
      "    Base image '86_1.JPG' has 21 variants.\n",
      "    Base image '114_1.JPG' has 21 variants.\n",
      "    Base image '151_1.JPG' has 21 variants.\n",
      "\n",
      "Annotations file saved as 'annotations.csv' with 127890 pairs.\n"
     ]
    }
   ],
   "source": [
    "bound_box_path = Path(\"../../datasets/BIVTatt-Dataset/bounding_boxes\")\n",
    "data_path = Path(\"../../datasets/BIVTatt-Dataset/images\")\n",
    "pattern = r'^\\d+_\\d+\\.JPG'\n",
    "total_bound_boxes = [file.name for file in bound_box_path.iterdir() if file.is_file()]\n",
    "all_images = [file.name for file in data_path.iterdir() if file.is_file()]\n",
    "base_images = [file.name for file in data_path.iterdir() if file.is_file() and re.match(pattern, file.name)]\n",
    "\n",
    "print (\"Base images in data folder: \")\n",
    "print(\"     Total of bounding boxes: \", len(total_bound_boxes))\n",
    "print(\"     Total of images: \", len(all_images))\n",
    "print(\"     Total of base images: \", len(base_images))\n",
    "print('')\n",
    "print(\"Base images and their variants\")\n",
    "\n",
    "base_image_variant_counts = {base_image: 0 for base_image in base_images}\n",
    "\n",
    "for image in all_images:\n",
    "    for base_image in base_images:\n",
    "        if image.startswith(base_image[:-4]):\n",
    "            base_image_variant_counts[base_image] += 1\n",
    "\n",
    "for base_image, count in base_image_variant_counts.items():\n",
    "    print(f\"    Base image '{base_image}' has {count} variants.\")\n",
    "\n",
    "# Define dataset base path\n",
    "dataset_path = annotations.bivtatt_dataset_path\n",
    "\n",
    "print(\"\")\n",
    "annotations.bivtatt_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Needed classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIVTattDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Generate pairs from the dataset\n",
    "        for file in os.listdir(data_dir):\n",
    "            if \"_\" in file:\n",
    "                base_name, _ = file.split(\"_\", 1)\n",
    "                for other_file in os.listdir(data_dir):\n",
    "                    if other_file.startswith(base_name) and file != other_file:\n",
    "                        self.pairs.append((file, other_file))\n",
    "                        # Label: 1 if transformations are of the same base image, else 0\n",
    "                        self.labels.append(1 if file.split(\"_\")[-1][0] == other_file.split(\"_\")[-1][0] else 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path = self.pairs[idx]\n",
    "        img1 = Image.open(os.path.join(self.data_dir, img1_path)).convert(\"RGB\")\n",
    "        img2 = Image.open(os.path.join(self.data_dir, img2_path)).convert(\"RGB\")\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetPairClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetPairClassifier, self).__init__()\n",
    "        # Load pretrained MobileNetV2\n",
    "        self.backbone = models.mobilenet_v2(pretrained=True)\n",
    "        # Modify the first layer to accept 6 input channels (for concatenated images)\n",
    "        self.backbone.features[0][0] = nn.Conv2d(6, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        # Replace the final classifier for binary classification\n",
    "        self.backbone.classifier[1] = nn.Linear(self.backbone.last_channel, 1)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        # Concatenate the two images along the channel dimension\n",
    "        x = torch.cat((img1, img2), dim=1)  # Shape: [B, 6, H, W]\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configurations\n",
    "# cuda - for nvidia gpus\n",
    "# mps - for macbook air\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \n",
    "                      \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 11\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = BIVTattDataset(dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = MobileNetPairClassifier().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for img1, img2, labels in train_loader:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(img1, img2).squeeze(1)  # Remove singleton dimension\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), f\"{dataset_path}bivtatt_mobilenet_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for inference\n",
    "model.load_state_dict(torch.load(f\"{dataset_path}bivtatt_mobilenet_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Function for testing a single pair\n",
    "def verify_tattoo(image1_path, image2_path):\n",
    "    img1 = Image.open(image1_path).convert(\"RGB\")\n",
    "    img2 = Image.open(image2_path).convert(\"RGB\")\n",
    "\n",
    "    img1 = transform(img1).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    img2 = transform(img2).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img1, img2).item()\n",
    "        similarity = torch.sigmoid(torch.tensor(output)).item()\n",
    "    \n",
    "    return \"Match\" if similarity > 0.5 else \"No Match\", similarity\n",
    "\n",
    "# Example usage\n",
    "result, similarity = verify_tattoo(f\"{dataset_path}1_1.JPG\", f\"{dataset_path}1_1_a1.JPG\")\n",
    "print(f\"Result: {result}, Similarity: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Total Time\n",
    "\n",
    "This show the total time of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time: 124.0 minutes and 3.15 seconds\n"
     ]
    }
   ],
   "source": [
    "# Sets the total time of execution\n",
    "end_time = time.time()\n",
    "helpers.calculate_execution_time(start_time, end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maestria-sandbox-M6nm_6C6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
