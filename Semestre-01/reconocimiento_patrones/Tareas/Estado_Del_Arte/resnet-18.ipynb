{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-18\n",
    "\n",
    "Start-of-the-art tattoo verification using Siamese Network\n",
    "\n",
    "#### References\n",
    "- [RestNet-18](https://pytorch.org/hub/pytorch_vision_resnet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will help us to measure the time it took for the whole\n",
    "# notebook to execute\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append('../../utils')\n",
    "import datasets\n",
    "importlib.reload(datasets)\n",
    "import helpers\n",
    "importlib.reload(helpers)\n",
    "import annotations\n",
    "importlib.reload(annotations)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base images in data folder: \n",
      "     Total of bounding boxes:  4410\n",
      "     Total of images:  4410\n",
      "     Total of base images:  161\n",
      "\n",
      "Base images and their variants\n",
      "    Base image '118_1.JPG' has 21 variants.\n",
      "    Base image '103_2.JPG' has 21 variants.\n",
      "    Base image '77_1.JPG' has 21 variants.\n",
      "    Base image '32_1.JPG' has 21 variants.\n",
      "    Base image '14_2.JPG' has 21 variants.\n",
      "    Base image '53_1.JPG' has 21 variants.\n",
      "    Base image '16_1.JPG' has 21 variants.\n",
      "    Base image '144_1.JPG' has 21 variants.\n",
      "    Base image '93_1.JPG' has 21 variants.\n",
      "    Base image '91_1.JPG' has 21 variants.\n",
      "    Base image '146_1.JPG' has 21 variants.\n",
      "    Base image '103_1.JPG' has 21 variants.\n",
      "    Base image '51_1.JPG' has 21 variants.\n",
      "    Base image '14_1.JPG' has 21 variants.\n",
      "    Base image '88_1.JPG' has 21 variants.\n",
      "    Base image '75_1.JPG' has 21 variants.\n",
      "    Base image '16_2.JPG' has 21 variants.\n",
      "    Base image '127_1.JPG' has 21 variants.\n",
      "    Base image '48_1.JPG' has 21 variants.\n",
      "    Base image '10_4.JPG' has 21 variants.\n",
      "    Base image '71_1.JPG' has 21 variants.\n",
      "    Base image '34_1.JPG' has 21 variants.\n",
      "    Base image '12_2.JPG' has 21 variants.\n",
      "    Base image '123_1.JPG' has 21 variants.\n",
      "    Base image '68_1.JPG' has 21 variants.\n",
      "    Base image '95_1.JPG' has 21 variants.\n",
      "    Base image '12_3.JPG' has 21 variants.\n",
      "    Base image '10_1.JPG' has 21 variants.\n",
      "    Base image '10_3.JPG' has 21 variants.\n",
      "    Base image '12_1.JPG' has 21 variants.\n",
      "    Base image '34_2.JPG' has 21 variants.\n",
      "    Base image '105_1.JPG' has 21 variants.\n",
      "    Base image '97_1.JPG' has 21 variants.\n",
      "    Base image '138_1.JPG' has 21 variants.\n",
      "    Base image '159_1.JPG' has 21 variants.\n",
      "    Base image '121_1.JPG' has 21 variants.\n",
      "    Base image '73_1.JPG' has 21 variants.\n",
      "    Base image '36_1.JPG' has 21 variants.\n",
      "    Base image '10_2.JPG' has 21 variants.\n",
      "    Base image '17_3.JPG' has 21 variants.\n",
      "    Base image '102_1.JPG' has 21 variants.\n",
      "    Base image '9_3.JPG' has 21 variants.\n",
      "    Base image '147_1.JPG' has 21 variants.\n",
      "    Base image '15_1.JPG' has 21 variants.\n",
      "    Base image '50_1.JPG' has 21 variants.\n",
      "    Base image '90_1.JPG' has 21 variants.\n",
      "    Base image '11_5.JPG' has 21 variants.\n",
      "    Base image '49_1.JPG' has 21 variants.\n",
      "    Base image '11_4.JPG' has 21 variants.\n",
      "    Base image '89_1.JPG' has 21 variants.\n",
      "    Base image '31_1.JPG' has 21 variants.\n",
      "    Base image '74_1.JPG' has 21 variants.\n",
      "    Base image '17_2.JPG' has 21 variants.\n",
      "    Base image '126_1.JPG' has 21 variants.\n",
      "    Base image '124_1.JPG' has 21 variants.\n",
      "    Base image '33_1.JPG' has 21 variants.\n",
      "    Base image '76_1.JPG' has 21 variants.\n",
      "    Base image '15_2.JPG' has 21 variants.\n",
      "    Base image '119_1.JPG' has 21 variants.\n",
      "    Base image '11_6.JPG' has 21 variants.\n",
      "    Base image '92_1.JPG' has 21 variants.\n",
      "    Base image '15_3.JPG' has 21 variants.\n",
      "    Base image '9_1.JPG' has 21 variants.\n",
      "    Base image '17_1.JPG' has 42 variants.\n",
      "    Base image '52_1.JPG' has 21 variants.\n",
      "    Base image '145_1.JPG' has 21 variants.\n",
      "    Base image '17_5.JPG' has 21 variants.\n",
      "    Base image '9_5.JPG' has 21 variants.\n",
      "    Base image '96_1.JPG' has 21 variants.\n",
      "    Base image '13_1.JPG' has 21 variants.\n",
      "    Base image '56_1.JPG' has 21 variants.\n",
      "    Base image '104_1.JPG' has 21 variants.\n",
      "    Base image '11_3.JPG' has 21 variants.\n",
      "    Base image '120_1.JPG' has 21 variants.\n",
      "    Base image '37_1.JPG' has 21 variants.\n",
      "    Base image '11_2.JPG' has 21 variants.\n",
      "    Base image '9_4.JPG' has 21 variants.\n",
      "    Base image '158_1.JPG' has 21 variants.\n",
      "    Base image '17_4.JPG' has 21 variants.\n",
      "    Base image '17_6.JPG' has 21 variants.\n",
      "    Base image '15_4.JPG' has 21 variants.\n",
      "    Base image '35_1.JPG' has 21 variants.\n",
      "    Base image '70_1.JPG' has 21 variants.\n",
      "    Base image '13_2.JPG' has 21 variants.\n",
      "    Base image '122_1.JPG' has 21 variants.\n",
      "    Base image '11_1.JPG' has 21 variants.\n",
      "    Base image '54_1.JPG' has 21 variants.\n",
      "    Base image '37_2.JPG' has 21 variants.\n",
      "    Base image '15_5.JPG' has 21 variants.\n",
      "    Base image '69_1.JPG' has 21 variants.\n",
      "    Base image '17_7.JPG' has 21 variants.\n",
      "    Base image '65_1.JPG' has 21 variants.\n",
      "    Base image '20_1.JPG' has 21 variants.\n",
      "    Base image '137_1.JPG' has 21 variants.\n",
      "    Base image '98_1.JPG' has 21 variants.\n",
      "    Base image '58_1.JPG' has 21 variants.\n",
      "    Base image '39_1.JPG' has 21 variants.\n",
      "    Base image '81_1.JPG' has 21 variants.\n",
      "    Base image '1_2.JPG' has 21 variants.\n",
      "    Base image '156_1.JPG' has 21 variants.\n",
      "    Base image '113_1.JPG' has 21 variants.\n",
      "    Base image '22_2.JPG' has 21 variants.\n",
      "    Base image '41_1.JPG' has 21 variants.\n",
      "    Base image '17_9.JPG' has 21 variants.\n",
      "    Base image '20_2.JPG' has 21 variants.\n",
      "    Base image '43_1.JPG' has 21 variants.\n",
      "    Base image '154_1.JPG' has 21 variants.\n",
      "    Base image '111_1.JPG' has 21 variants.\n",
      "    Base image '83_1.JPG' has 21 variants.\n",
      "    Base image '1_1.JPG' has 21 variants.\n",
      "    Base image '135_1.JPG' has 21 variants.\n",
      "    Base image '22_1.JPG' has 21 variants.\n",
      "    Base image '17_8.JPG' has 21 variants.\n",
      "    Base image '24_2.JPG' has 21 variants.\n",
      "    Base image '47_1.JPG' has 21 variants.\n",
      "    Base image '115_1.JPG' has 21 variants.\n",
      "    Base image '19_2.JPG' has 21 variants.\n",
      "    Base image '128_1.JPG' has 21 variants.\n",
      "    Base image '87_1.JPG' has 21 variants.\n",
      "    Base image '85_1.JPG' has 21 variants.\n",
      "    Base image '78_1.JPG' has 21 variants.\n",
      "    Base image '152_1.JPG' has 21 variants.\n",
      "    Base image '117_1.JPG' has 21 variants.\n",
      "    Base image '45_1.JPG' has 21 variants.\n",
      "    Base image '61_1.JPG' has 21 variants.\n",
      "    Base image '24_1.JPG' has 21 variants.\n",
      "    Base image '19_1.JPG' has 21 variants.\n",
      "    Base image '82_1.JPG' has 21 variants.\n",
      "    Base image '21_2.JPG' has 21 variants.\n",
      "    Base image '110_1.JPG' has 21 variants.\n",
      "    Base image '155_1.JPG' has 21 variants.\n",
      "    Base image '134_1.JPG' has 21 variants.\n",
      "    Base image '66_1.JPG' has 21 variants.\n",
      "    Base image '21_3.JPG' has 21 variants.\n",
      "    Base image '80_2.JPG' has 21 variants.\n",
      "    Base image '2_1.JPG' has 21 variants.\n",
      "    Base image '21_1.JPG' has 21 variants.\n",
      "    Base image '64_1.JPG' has 21 variants.\n",
      "    Base image '136_1.JPG' has 21 variants.\n",
      "    Base image '99_1.JPG' has 21 variants.\n",
      "    Base image '112_1.JPG' has 21 variants.\n",
      "    Base image '157_1.JPG' has 21 variants.\n",
      "    Base image '40_1.JPG' has 21 variants.\n",
      "    Base image '38_1.JPG' has 21 variants.\n",
      "    Base image '80_1.JPG' has 21 variants.\n",
      "    Base image '116_1.JPG' has 21 variants.\n",
      "    Base image '153_1.JPG' has 21 variants.\n",
      "    Base image '17_10.JPG' has 21 variants.\n",
      "    Base image '44_1.JPG' has 21 variants.\n",
      "    Base image '84_1.JPG' has 21 variants.\n",
      "    Base image '21_4.JPG' has 21 variants.\n",
      "    Base image '79_1.JPG' has 21 variants.\n",
      "    Base image '6_1.JPG' has 21 variants.\n",
      "    Base image '132_1.JPG' has 21 variants.\n",
      "    Base image '130_1.JPG' has 21 variants.\n",
      "    Base image '148_1.JPG' has 21 variants.\n",
      "    Base image '4_1.JPG' has 21 variants.\n",
      "    Base image '129_1.JPG' has 21 variants.\n",
      "    Base image '86_1.JPG' has 21 variants.\n",
      "    Base image '114_1.JPG' has 21 variants.\n",
      "    Base image '151_1.JPG' has 21 variants.\n"
     ]
    }
   ],
   "source": [
    "bound_box_path = Path(\"../../datasets/BIVTatt-Dataset/bounding_boxes\")\n",
    "data_path = Path(\"../../datasets/BIVTatt-Dataset/images\")\n",
    "pattern = r'^\\d+_\\d+\\.JPG'\n",
    "total_bound_boxes = [file.name for file in bound_box_path.iterdir() if file.is_file()]\n",
    "all_images = [file.name for file in data_path.iterdir() if file.is_file()]\n",
    "base_images = [file.name for file in data_path.iterdir() if file.is_file() and re.match(pattern, file.name)]\n",
    "\n",
    "print (\"Base images in data folder: \")\n",
    "print(\"     Total of bounding boxes: \", len(total_bound_boxes))\n",
    "print(\"     Total of images: \", len(all_images))\n",
    "print(\"     Total of base images: \", len(base_images))\n",
    "print('')\n",
    "print(\"Base images and their variants\")\n",
    "\n",
    "base_image_variant_counts = {base_image: 0 for base_image in base_images}\n",
    "\n",
    "for image in all_images:\n",
    "    for base_image in base_images:\n",
    "        if image.startswith(base_image[:-4]):\n",
    "            base_image_variant_counts[base_image] += 1\n",
    "\n",
    "for base_image, count in base_image_variant_counts.items():\n",
    "    print(f\"    Base image '{base_image}' has {count} variants.\")\n",
    "\n",
    "# Define dataset base path\n",
    "dataset_path = annotations.bivtatt_dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIVTattDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load all image pairs and labels\n",
    "        for file in os.listdir(data_dir):\n",
    "            if \"_\" in file:\n",
    "                base_name, _ = file.split(\"_\", 1)\n",
    "                for other_file in os.listdir(data_dir):\n",
    "                    if other_file.startswith(base_name) and file != other_file:\n",
    "                        self.pairs.append((file, other_file))\n",
    "                        self.labels.append(1 if file.split(\"_\")[-1][0] == other_file.split(\"_\")[-1][0] else 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path = self.pairs[idx]\n",
    "        img1 = Image.open(os.path.join(self.data_dir, img1_path)).convert(\"RGB\")\n",
    "        img2 = Image.open(os.path.join(self.data_dir, img2_path)).convert(\"RGB\")\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePairClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImagePairClassifier, self).__init__()\n",
    "        # Load pretrained ResNet-18\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        # Adjust input channels for concatenated images\n",
    "        self.backbone.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # Update the final fully connected layer for binary classification\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, 1)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        # Concatenate the two images along the channel dimension\n",
    "        x = torch.cat((img1, img2), dim=1)  # Shape: [B, 6, H, W]\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/administrator/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/administrator/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "# cuda - for nvidia gpus\n",
    "# mps - for macbook air\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \n",
    "                      \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = BIVTattDataset(f\"{dataset_path}images\", transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = ImagePairClassifier().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      4\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[21], line 28\u001b[0m, in \u001b[0;36mBIVTattDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     27\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img1)\n\u001b[0;32m---> 28\u001b[0m     img2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img1, img2, label\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/maestria-sandbox-M6nm_6C6/lib/python3.12/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for img1, img2, labels in train_loader:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(img1, img2).squeeze(1)  # Remove singleton dimension\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), f\"{dataset_path}bivtatt_resnet_18_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for inference\n",
    "model.load_state_dict(torch.load(f\"{dataset_path}bivtatt_resnet_18_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Function for testing a single pair\n",
    "def verify_tattoo(image1_path, image2_path):\n",
    "    img1 = Image.open(image1_path).convert(\"RGB\")\n",
    "    img2 = Image.open(image2_path).convert(\"RGB\")\n",
    "\n",
    "    img1 = transform(img1).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    img2 = transform(img2).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img1, img2).item()\n",
    "        similarity = torch.sigmoid(torch.tensor(output)).item()\n",
    "    \n",
    "    return \"Match\" if similarity > 0.5 else \"No Match\", similarity\n",
    "\n",
    "# Example usage\n",
    "result, similarity = verify_tattoo(f\"{dataset_path}1_1.JPG\", f\"{dataset_path}1_1_a1.JPG\")\n",
    "print(f\"Result: {result}, Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Total Time\n",
    "\n",
    "This show the total time of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the total time of execution\n",
    "end_time = time.time()\n",
    "helpers.calculate_execution_time(start_time, end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maestria-sandbox-M6nm_6C6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
